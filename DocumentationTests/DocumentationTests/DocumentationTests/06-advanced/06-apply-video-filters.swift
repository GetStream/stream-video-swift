import Foundation
import StreamVideo
import StreamVideoSwiftUI
import CoreImage
import SwiftUI
import Vision
import StreamWebRTC

@MainActor
fileprivate func content() {
    container {
        let sepia: VideoFilter = {
            let sepia = VideoFilter(id: "sepia", name: "Sepia") { input in
                let sepiaFilter = CIFilter(name: "CISepiaTone")
                sepiaFilter?.setValue(input.originalImage, forKey: kCIInputImageKey)
                return sepiaFilter?.outputImage ?? input.originalImage
            }
            return sepia
        }()
    }

    container {

        class FiltersService: ObservableObject {
            @Published var filtersShown = false
            @Published var selectedFilter: VideoFilter?

            static let supportedFilters = [sepia]
        }

        let streamVideo = StreamVideo(
            apiKey: apiKey,
            user: userCredentials.user,
            token: token,
            // highlight-start
            videoConfig: VideoConfig(
                videoFilters: FiltersService.supportedFilters
            ),
            // highlight-end
            tokenProvider: { result in
                // Unrelated code skipped. Check repository for complete code:
                // https://github.com/GetStream/stream-video-ios-examples/blob/main/VideoWithChat/VideoWithChat/StreamWrapper.swift
            }
        )
        connectUser()

        func connectUser() {
            Task {
                try await streamVideo.connect()
            }
        }
    }

    container {
        class VideoViewFactory: ViewFactory {

            func makeOutgoingCallView(viewModel: CallViewModel) -> some View {
                CustomOutgoingCallView(viewModel: viewModel)
            }
        }
    }

    container {
        struct ChatCallControls: View {
            var body: some View {
                VStack {
                    HStack {
                        /* Skip unrelated code */
                        // highlight-next-line
                        // 1. Button to toggle filters view
                        Button {
                            withAnimation {
                                filtersService.filtersShown.toggle()
                            }
                        } label: {
                            CallIconView(
                                icon: Image(systemName: "camera.filters"),
                                size: size,
                                iconStyle: filtersService.filtersShown ? .primary : .transparent
                            )
                        }
                        /* Skip unrelated code */
                    }

                    if filtersService.filtersShown {
                        HStack(spacing: 16) {
                            // highlight-next-line
                            // 2. Show a button for each filter
                            ForEach(FiltersService.supportedFilters, id: \.id) { filter in
                                Button {
                                    withAnimation {
                                        // highlight-next-line
                                        // 3. Select or de-select filter on tap
                                        if filtersService.selectedFilter?.id == filter.id {
                                            filtersService.selectedFilter = nil
                                        } else {
                                            filtersService.selectedFilter = filter
                                        }
                                        viewModel.setVideoFilter(filtersService.selectedFilter)
                                    }
                                } label: {
                                    Text(filter.name)
                                        .background(filtersService.selectedFilter?.id == filter.id ? Color.blue : Color.gray)
                                    /* more modifiers */
                                }

                            }
                        }
                    }
                }
                /* more modifiers */
            }
        }
    }

    container {
        func detectFaces(image: CIImage) async throws -> CGRect {
            return try await withCheckedThrowingContinuation { continuation in
                let detectFaceRequest = VNDetectFaceRectanglesRequest { (request, error) in
                    if let result = request.results?.first as? VNFaceObservation {
                        continuation.resume(returning: result.boundingBox)
                    } else {
                        continuation.resume(throwing: ClientError.Unknown())
                    }
                }
                let vnImage = VNImageRequestHandler(ciImage: image, orientation: .downMirrored)
                try? vnImage.perform([detectFaceRequest])
            }
        }

        func convert(ciImage: CIImage) -> UIImage {
            let context = CIContext(options: nil)
            let cgImage = context.createCGImage(ciImage, from: ciImage.extent)!
            let image = UIImage(cgImage: cgImage, scale: 1, orientation: .up)
            return image
        }

        @MainActor
        func drawImageIn(_ image: UIImage, size: CGSize, _ logo: UIImage, inRect: CGRect) -> UIImage {
            let format = UIGraphicsImageRendererFormat()
            format.scale = 1
            format.opaque = true
            let renderer = UIGraphicsImageRenderer(size: size, format: format)
            return renderer.image { context in
                image.draw(in: CGRect(origin: CGPoint.zero, size: size))
                logo.draw(in: inRect)
            }
        }

        let stream: VideoFilter = {
            let stream = VideoFilter(id: "stream", name: "Stream") { input in
                // highlight-next-line
                // 1. detect, where the face is located (if there's any)
                guard let faceRect = try? await detectFaces(image: input.originalImage) else { return input.originalImage }
                let converted = convert(ciImage: input.originalImage)
                let bounds = input.originalImage.extent
                let convertedRect = CGRect(
                    x: faceRect.minX * bounds.width - 80,
                    y: faceRect.minY * bounds.height,
                    width: faceRect.width * bounds.width,
                    height: faceRect.height * bounds.height
                )
                // highlight-next-line
                // 2. Overlay the rectangle onto the original image
                let overlayed = drawImageIn(converted, size: bounds.size, streamLogo, inRect: convertedRect)

                // highlight-next-line
                // 3. convert the created image to a CIImage
                let result = CIImage(cgImage: overlayed.cgImage!)
                return result
            }
            return stream
        }()
    }

    viewContainer {
        Button {
            call.setVideoFilter(.blurredBackground)
        } label: {
            Text("Apply blur background filter")
        }

        Button {
            call.setVideoFilter(nil)
        } label: {
            Text("Remove background filter")
        }
    }

    viewContainer {
        Button {
            call.setVideoFilter(.imageBackground(CIImage(image: uiImage)!, id: "my-awesome-image-background-filter"))
        } label: {
            Text("Apply image background filter")
        }

        Button {
            call.setVideoFilter(nil)
        } label: {
            Text("Remove background filter")
        }
    }

    container {
        let voiceProcessor = CustomVoiceProcessor()
        let filter = RobotVoiceFilter(pitchShift: 0.8)
        voiceProcessor.setAudioFilter(filter)
        let audioProcessingModule = RTCDefaultAudioProcessingModule(
            config: nil,
            capturePostProcessingDelegate: voiceProcessor,
            renderPreProcessingDelegate: nil
        )
    }

    container {
        class CustomVoiceProcessor: NSObject, RTCAudioCustomProcessingDelegate {

            private var audioFilter: AudioFilter?

            func audioProcessingInitialize(sampleRate sampleRateHz: Int, channels: Int) {}

            func audioProcessingProcess(audioBuffer: RTCAudioBuffer) {
                var audioBuffer = audioBuffer
                audioFilter?.applyEffect(to: &audioBuffer)
            }

            func audioProcessingRelease() {}

            func setAudioFilter(_ audioFilter: AudioFilter?) {
                self.audioFilter = audioFilter
            }
        }
    }

    container {
        class RobotVoiceFilter: AudioFilter {

            let pitchShift: Float

            init(pitchShift: Float) {
                self.pitchShift = pitchShift
            }

            func applyEffect(to audioBuffer: inout RTCAudioBuffer) {
                let frameSize = 256
                let hopSize = 128
                let scaleFactor = Float(frameSize) / Float(hopSize)

                let numFrames = (audioBuffer.frames - frameSize) / hopSize

                for channel in 0..<audioBuffer.channels {
                    let channelBuffer = audioBuffer.rawBuffer(forChannel: channel)

                    for i in 0..<numFrames {
                        let inputOffset = i * hopSize
                        let outputOffset = Int(Float(i) * scaleFactor) * hopSize

                        var outputFrame = [Float](repeating: 0.0, count: frameSize)

                        // Apply pitch shift
                        for j in 0..<frameSize {
                            let shiftedIndex = Int(Float(j) * pitchShift)
                            let originalIndex = inputOffset + j
                            if shiftedIndex >= 0 && shiftedIndex < frameSize && originalIndex >= 0 && originalIndex < audioBuffer.frames {
                                outputFrame[shiftedIndex] = channelBuffer[originalIndex]
                            }
                        }

                        // Copy back to the input buffer
                        for j in 0..<frameSize {
                            let outputIndex = outputOffset + j
                            if outputIndex >= 0 && outputIndex < audioBuffer.frames {
                                channelBuffer[outputIndex] = outputFrame[j]
                            }
                        }
                    }
                }
            }
        }
    }
}
